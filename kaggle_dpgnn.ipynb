{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DPGNN - Dual-Perspective Graph Neural Network\n",
    "## Person-Job Fit on v5-sections Dataset\n",
    "\n",
    "Paper: \"Modeling Two-Way Selection Preference for Person-Job Fit\" (RecSys 2022)\n",
    "\n",
    "**Instructions:**\n",
    "1. Upload your `train.csv` and `test.csv` to Kaggle\n",
    "2. Enable GPU (T4 x2)\n",
    "3. Run all cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install torch_geometric -q\n",
    "!pip install transformers -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone original DPGNN repository\n",
    "!git clone https://github.com/RUCAIBox/DPGNN.git\n",
    "%cd DPGNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"Device count: {torch.cuda.device_count()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Upload & Convert Dataset\n",
    "\n",
    "Upload your `train.csv` and `test.csv` files to `/kaggle/input/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update this path to match your uploaded dataset\n",
    "TRAIN_CSV = \"/kaggle/input/workfitai-v5-sections/train.csv\"  # Change this\n",
    "TEST_CSV = \"/kaggle/input/workfitai-v5-sections/test.csv\"    # Change this\n",
    "\n",
    "# Check if files exist\n",
    "print(f\"Train exists: {os.path.exists(TRAIN_CSV)}\")\n",
    "print(f\"Test exists: {os.path.exists(TEST_CSV)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train_df = pd.read_csv(TRAIN_CSV).fillna('')\n",
    "test_df = pd.read_csv(TEST_CSV).fillna('')\n",
    "\n",
    "print(f\"Train samples: {len(train_df)}\")\n",
    "print(f\"Test samples: {len(test_df)}\")\n",
    "print(f\"\\nColumns: {list(train_df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset directory\n",
    "os.makedirs('dataset', exist_ok=True)\n",
    "\n",
    "# Define columns\n",
    "resume_cols = ['resume_summary', 'resume_experience', 'resume_skills', 'resume_education']\n",
    "job_cols = ['jd_overview', 'jd_responsibilities', 'jd_requirements', 'jd_preferred']\n",
    "\n",
    "def combine_sections(row, cols):\n",
    "    texts = []\n",
    "    for col in cols:\n",
    "        text = str(row.get(col, ''))\n",
    "        if text and text.lower() != 'nan' and text.strip():\n",
    "            texts.append(text)\n",
    "    return ' [SEP] '.join(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build unique geeks (resumes)\n",
    "print(\"Building unique geeks...\")\n",
    "all_df = pd.concat([train_df, test_df], ignore_index=True)\n",
    "\n",
    "geek_texts = {}\n",
    "for idx, row in tqdm(all_df.iterrows(), total=len(all_df)):\n",
    "    geek_token = str(row['original_index'])\n",
    "    if geek_token not in geek_texts:\n",
    "        geek_texts[geek_token] = combine_sections(row, resume_cols)\n",
    "\n",
    "geek_tokens = sorted(geek_texts.keys(), key=lambda x: int(x))\n",
    "print(f\"Found {len(geek_tokens)} unique geeks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build unique jobs\n",
    "print(\"Building unique jobs...\")\n",
    "job_hashes = {}\n",
    "job_texts = {}\n",
    "\n",
    "for idx, row in tqdm(all_df.iterrows(), total=len(all_df)):\n",
    "    jd_content = '|'.join([str(row.get(col, '')) for col in job_cols])\n",
    "    jd_hash = hash(jd_content)\n",
    "    \n",
    "    if jd_hash not in job_hashes:\n",
    "        job_token = str(len(job_hashes))\n",
    "        job_hashes[jd_hash] = job_token\n",
    "        job_texts[job_token] = combine_sections(row, job_cols)\n",
    "\n",
    "job_tokens = sorted(job_texts.keys(), key=lambda x: int(x))\n",
    "print(f\"Found {len(job_tokens)} unique jobs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write token files\n",
    "print(\"Writing token files...\")\n",
    "with open('dataset/geek.token', 'w') as f:\n",
    "    for token in geek_tokens:\n",
    "        f.write(f\"{token}\\n\")\n",
    "\n",
    "with open('dataset/job.token', 'w') as f:\n",
    "    for token in job_tokens:\n",
    "        f.write(f\"{token}\\n\")\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process interactions\n",
    "print(\"Processing interactions...\")\n",
    "\n",
    "def get_job_token(row, job_cols, job_hashes):\n",
    "    jd_content = '|'.join([str(row.get(col, '')) for col in job_cols])\n",
    "    jd_hash = hash(jd_content)\n",
    "    return job_hashes[jd_hash]\n",
    "\n",
    "# Training data\n",
    "train_interactions = []\n",
    "positive_interactions = []\n",
    "\n",
    "for idx, row in tqdm(train_df.iterrows(), total=len(train_df)):\n",
    "    geek_token = str(row['original_index'])\n",
    "    job_token = get_job_token(row, job_cols, job_hashes)\n",
    "    label = 1 if str(row['label']).strip().lower() == 'good fit' else 0\n",
    "    \n",
    "    train_interactions.append((geek_token, job_token, label))\n",
    "    if label == 1:\n",
    "        positive_interactions.append((geek_token, job_token, label))\n",
    "\n",
    "print(f\"Total train: {len(train_interactions)}\")\n",
    "print(f\"Positive train: {len(positive_interactions)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create validation split\n",
    "np.random.seed(42)\n",
    "n_valid = int(len(positive_interactions) * 0.1)\n",
    "indices = np.random.permutation(len(positive_interactions))\n",
    "valid_indices = set(indices[:n_valid])\n",
    "\n",
    "valid_interactions = []\n",
    "train_final = []\n",
    "\n",
    "for i, inter in enumerate(positive_interactions):\n",
    "    if i in valid_indices:\n",
    "        valid_interactions.append(inter)\n",
    "    else:\n",
    "        train_final.append(inter)\n",
    "\n",
    "print(f\"Train final: {len(train_final)}\")\n",
    "print(f\"Validation: {len(valid_interactions)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process test data\n",
    "test_interactions = []\n",
    "for idx, row in tqdm(test_df.iterrows(), total=len(test_df)):\n",
    "    geek_token = str(row['original_index'])\n",
    "    job_token = get_job_token(row, job_cols, job_hashes)\n",
    "    label = 1 if str(row['label']).strip().lower() == 'good fit' else 0\n",
    "    test_interactions.append((geek_token, job_token, label))\n",
    "\n",
    "print(f\"Test: {len(test_interactions)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write interaction files\n",
    "def write_interactions(filepath, interactions):\n",
    "    with open(filepath, 'w') as f:\n",
    "        for geek, job, label in interactions:\n",
    "            f.write(f\"{geek}\\t{job}\\t{label}\\n\")\n",
    "\n",
    "write_interactions('dataset/data.train_all', train_final)\n",
    "write_interactions('dataset/data.train_all_add', train_final)\n",
    "write_interactions('dataset/data.user_add', train_final)  # Same as train for our case\n",
    "write_interactions('dataset/data.job_add', train_final)\n",
    "write_interactions('dataset/data.valid_g', valid_interactions)\n",
    "write_interactions('dataset/data.valid_j', valid_interactions)\n",
    "write_interactions('dataset/data.test_g', test_interactions)\n",
    "write_interactions('dataset/data.test_j', test_interactions)\n",
    "\n",
    "print(\"Interaction files written!\")\n",
    "!ls -la dataset/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generate BERT Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load BERT\n",
    "print(\"Loading BERT model...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = AutoModel.from_pretrained('bert-base-uncased').to(device)\n",
    "model.eval()\n",
    "print(\"BERT loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_texts_bert(texts, batch_size=32, max_length=512):\n",
    "    all_embeddings = []\n",
    "    \n",
    "    for i in tqdm(range(0, len(texts), batch_size)):\n",
    "        batch_texts = texts[i:i + batch_size]\n",
    "        \n",
    "        encoded = tokenizer(\n",
    "            batch_texts,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        input_ids = encoded['input_ids'].to(device)\n",
    "        attention_mask = encoded['attention_mask'].to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "        \n",
    "        all_embeddings.append(embeddings)\n",
    "        \n",
    "        # Clear GPU memory\n",
    "        del input_ids, attention_mask, outputs\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    return np.vstack(all_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode geeks (resumes)\n",
    "print(\"Encoding geeks...\")\n",
    "geek_text_list = [geek_texts[t] for t in geek_tokens]\n",
    "geek_embeddings = encode_texts_bert(geek_text_list, batch_size=32)\n",
    "\n",
    "# Format: first column is ID, rest is embedding\n",
    "geek_ids = np.array([int(t) for t in geek_tokens]).reshape(-1, 1)\n",
    "geek_data = np.hstack([geek_ids, geek_embeddings])\n",
    "np.save('dataset/geek.bert.npy', geek_data)\n",
    "print(f\"Saved geek.bert.npy: shape {geek_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode jobs\n",
    "print(\"Encoding jobs...\")\n",
    "job_text_list = [job_texts[t] for t in job_tokens]\n",
    "job_embeddings = encode_texts_bert(job_text_list, batch_size=32)\n",
    "\n",
    "# Format: first column is ID, rest is embedding\n",
    "job_ids_arr = np.array([int(t) for t in job_tokens]).reshape(-1, 1)\n",
    "job_data = np.hstack([job_ids_arr, job_embeddings])\n",
    "np.save('dataset/job.bert.npy', job_data)\n",
    "print(f\"Saved job.bert.npy: shape {job_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify dataset files\n",
    "print(\"Dataset files:\")\n",
    "!ls -la dataset/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Update Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update overall.yaml\n",
    "overall_config = \"\"\"# Device\n",
    "use_gpu: True\n",
    "gpu_id: 0\n",
    "\n",
    "# Training\n",
    "learner: Adam\n",
    "epochs: 50\n",
    "eval_step: 1\n",
    "stopping_step: 10\n",
    "clip_grad_norm: ~\n",
    "\n",
    "# Evaluation\n",
    "topk: [5]\n",
    "valid_metric: r@5\n",
    "\n",
    "# DataLoader\n",
    "num_workers: 2\n",
    "pin_memory: True\n",
    "\n",
    "# General\n",
    "checkpoint_dir: ./saved/\n",
    "dataset_path: ./dataset/\n",
    "\n",
    "loss_decimal_place: 4\n",
    "metric_decimal_place: 4\n",
    "\n",
    "# Reproducibility\n",
    "seed: 42\n",
    "reproducibility: True\n",
    "\"\"\"\n",
    "\n",
    "with open('prop/overall.yaml', 'w') as f:\n",
    "    f.write(overall_config)\n",
    "print(\"Updated prop/overall.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update DPGNN.yaml\n",
    "dpgnn_config = \"\"\"# Model\n",
    "embedding_size: 128\n",
    "n_layers: 3\n",
    "reg_weight: 1e-05\n",
    "mutual_weight: 0.05\n",
    "temperature: 0.2\n",
    "\n",
    "ADD_BERT: True\n",
    "BERT_embedding_size: 768\n",
    "BERT_output_size: 32\n",
    "\n",
    "# Training\n",
    "learning_rate: 0.001\n",
    "\n",
    "# Batch size - adjusted for T4 GPU\n",
    "train_batch_size: 512\n",
    "eval_batch_size: 512\n",
    "\"\"\"\n",
    "\n",
    "with open('prop/DPGNN.yaml', 'w') as f:\n",
    "    f.write(dpgnn_config)\n",
    "print(\"Updated prop/DPGNN.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train DPGNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear GPU memory before training\n",
    "del model, tokenizer\n",
    "torch.cuda.empty_cache()\n",
    "import gc\n",
    "gc.collect()\n",
    "print(\"GPU memory cleared!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training\n",
    "!python main.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluate & Get Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check saved models\n",
    "!ls -la saved/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print final results summary\n",
    "print(\"=\"*60)\n",
    "print(\"DPGNN Training Complete!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nDataset: v5-sections\")\n",
    "print(f\"Unique geeks: {len(geek_tokens)}\")\n",
    "print(f\"Unique jobs: {len(job_tokens)}\")\n",
    "print(f\"Training samples: {len(train_final)}\")\n",
    "print(f\"Test samples: {len(test_interactions)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes for Paper\n",
    "\n",
    "When reporting DPGNN results:\n",
    "\n",
    "```\n",
    "We used the official DPGNN implementation from [1] with the following adaptations:\n",
    "- Dataset converted from v5-sections format to DPGNN's expected format\n",
    "- BERT embeddings generated using bert-base-uncased\n",
    "- Training: 50 epochs, batch size 512, learning rate 0.001\n",
    "- 3 GCN layers, embedding size 128\n",
    "\n",
    "[1] Yang et al. \"Modeling Two-Way Selection Preference for Person-Job Fit\" RecSys 2022\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
